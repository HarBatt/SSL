{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e25538",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from itertools import chain\n",
    "from utils import batch_generator\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b38069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    \"\"\"Min Max normalizer.\n",
    "\n",
    "    Args:\n",
    "    - data: original data\n",
    "\n",
    "    Returns:\n",
    "    - norm_data: normalized data\n",
    "    \"\"\"\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    norm_data = numerator / (denominator + 1e-7)\n",
    "    return norm_data\n",
    "\n",
    "def StandardizationScaler(data):\n",
    "    mean, std = np.mean(data, 0), np.mean(data, 0)\n",
    "    norm_data = (data - mean)/(std + 1e-7)\n",
    "    return norm_data\n",
    "\n",
    "  \n",
    "def real_data_loading(absolute_path):\n",
    "    \"\"\"Load and preprocess real-world datasets.\n",
    "  \n",
    "    Args:\n",
    "    - absolute_path: absoulte_path\n",
    "    - seq_len: sequence length\n",
    "\n",
    "    Returns:\n",
    "    - data: preprocessed data.\n",
    "    \"\"\"  \n",
    "    ori_data = pd.read_csv(absolute_path, index_col= 0)\n",
    "    cols = [\"subset\"]\n",
    "    ori_data = ori_data.drop(columns = cols)\n",
    "    ori_data = ori_data.values\n",
    "    # First dimension is time or last dimension is label #ori_data = ori_data[:, :-1]\n",
    "    ori_data = ori_data.astype(dtype = np.float64)\n",
    "    # Normalize the data\n",
    "    ori_data = MinMaxScaler(ori_data)\n",
    "    return np.array(ori_data)\n",
    "\n",
    "\n",
    "\n",
    "def compute_measures(y_tilde, y):\n",
    "    y_pred = np.round(y_tilde.to('cpu').detach(), decimals = 0)\n",
    "    y = np.round(y.to('cpu').detach(), decimals = 0)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    return acc, prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428bafbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (fc1): Linear(in_features=196, out_features=96, bias=True)\n",
       "  (fc2): Linear(in_features=96, out_features=96, bias=True)\n",
       "  (fc3): Linear(in_features=96, out_features=1, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set the device \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "## Data loading\n",
    "data_path = \"data/\"\n",
    "path_train_data = data_path + 'train_data.csv'\n",
    "path_valid_data = data_path + 'valid_data.csv'\n",
    "path_test_data = data_path + 'test_data.csv'\n",
    "\n",
    "#preprocessing the data.\n",
    "train_data = real_data_loading(path_train_data)   \n",
    "valid_data = real_data_loading(path_valid_data)\n",
    "test_data = real_data_loading(path_test_data)\n",
    "disc = torch.load(\"pre_trained_model/discriminator.mdl\")\n",
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eafcae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=196, out_features=96, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.2)\n",
       "  (2): Dropout(p=0.2, inplace=False)\n",
       "  (3): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): Linear(in_features=96, out_features=1, bias=True)\n",
       "  (5): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.ModuleList([])\n",
    "for index, layer in enumerate(disc.children()):\n",
    "    if index==1:\n",
    "        model.append(nn.LeakyReLU(0.2))\n",
    "        model.append(nn.Dropout(0.2))\n",
    "        model.append(nn.BatchNorm1d(96))\n",
    "    else: \n",
    "        if not isinstance(layer, torch.nn.LeakyReLU):\n",
    "            model.append(layer)\n",
    "model.append(nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af223deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=196, out_features=96, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(in_features=96, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bn = nn.BatchNorm1d(96)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c4bcd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Model()\\nmodel = model.to(device)\\nmodel\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(*model).to(device)\n",
    "model\n",
    "\"\"\"\n",
    "model = Model()\n",
    "model = model.to(device)\n",
    "model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29952daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9752e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; train_loss: 0.745; val_loss: 1.05; train_acc: 0.518; train_f1: 0.549, val_acc: 0.6; val_f1: 0.692\n",
      "Epoch 1; train_loss: 0.736; val_loss: 1.009; train_acc: 0.515; train_f1: 0.543, val_acc: 0.608; val_f1: 0.733\n",
      "Epoch 2; train_loss: 0.73; val_loss: 1.02; train_acc: 0.52; train_f1: 0.55, val_acc: 0.55; val_f1: 0.663\n",
      "Epoch 3; train_loss: 0.725; val_loss: 1.085; train_acc: 0.532; train_f1: 0.568, val_acc: 0.6; val_f1: 0.717\n",
      "Epoch 4; train_loss: 0.718; val_loss: 1.009; train_acc: 0.545; train_f1: 0.583, val_acc: 0.55; val_f1: 0.655\n",
      "Epoch 5; train_loss: 0.704; val_loss: 0.925; train_acc: 0.569; train_f1: 0.593, val_acc: 0.608; val_f1: 0.718\n",
      "Epoch 6; train_loss: 0.705; val_loss: 1.03; train_acc: 0.544; train_f1: 0.576, val_acc: 0.583; val_f1: 0.714\n",
      "Epoch 7; train_loss: 0.686; val_loss: 0.849; train_acc: 0.591; train_f1: 0.623, val_acc: 0.608; val_f1: 0.717\n",
      "Epoch 8; train_loss: 0.691; val_loss: 1.031; train_acc: 0.564; train_f1: 0.596, val_acc: 0.55; val_f1: 0.651\n",
      "Epoch 9; train_loss: 0.679; val_loss: 1.13; train_acc: 0.588; train_f1: 0.616, val_acc: 0.542; val_f1: 0.658\n",
      "Epoch 10; train_loss: 0.688; val_loss: 0.881; train_acc: 0.577; train_f1: 0.607, val_acc: 0.6; val_f1: 0.709\n",
      "Epoch 11; train_loss: 0.663; val_loss: 0.979; train_acc: 0.613; train_f1: 0.641, val_acc: 0.55; val_f1: 0.666\n",
      "Epoch 12; train_loss: 0.66; val_loss: 0.846; train_acc: 0.615; train_f1: 0.641, val_acc: 0.617; val_f1: 0.65\n",
      "Epoch 13; train_loss: 0.654; val_loss: 0.871; train_acc: 0.619; train_f1: 0.645, val_acc: 0.617; val_f1: 0.718\n",
      "Epoch 14; train_loss: 0.647; val_loss: 1.029; train_acc: 0.638; train_f1: 0.655, val_acc: 0.542; val_f1: 0.649\n",
      "Epoch 15; train_loss: 0.642; val_loss: 0.904; train_acc: 0.627; train_f1: 0.652, val_acc: 0.55; val_f1: 0.654\n",
      "Epoch 16; train_loss: 0.635; val_loss: 0.811; train_acc: 0.638; train_f1: 0.663, val_acc: 0.625; val_f1: 0.712\n",
      "Epoch 17; train_loss: 0.637; val_loss: 0.741; train_acc: 0.64; train_f1: 0.663, val_acc: 0.625; val_f1: 0.68\n",
      "Epoch 18; train_loss: 0.616; val_loss: 0.757; train_acc: 0.658; train_f1: 0.684, val_acc: 0.583; val_f1: 0.658\n",
      "Epoch 19; train_loss: 0.615; val_loss: 0.948; train_acc: 0.667; train_f1: 0.687, val_acc: 0.558; val_f1: 0.662\n",
      "Epoch 20; train_loss: 0.615; val_loss: 0.925; train_acc: 0.671; train_f1: 0.683, val_acc: 0.542; val_f1: 0.652\n",
      "Epoch 21; train_loss: 0.606; val_loss: 0.753; train_acc: 0.676; train_f1: 0.695, val_acc: 0.617; val_f1: 0.711\n",
      "Epoch 22; train_loss: 0.616; val_loss: 0.83; train_acc: 0.657; train_f1: 0.673, val_acc: 0.55; val_f1: 0.635\n",
      "Epoch 23; train_loss: 0.6; val_loss: 0.768; train_acc: 0.67; train_f1: 0.69, val_acc: 0.617; val_f1: 0.71\n",
      "Epoch 24; train_loss: 0.609; val_loss: 0.735; train_acc: 0.675; train_f1: 0.69, val_acc: 0.633; val_f1: 0.722\n",
      "Epoch 25; train_loss: 0.591; val_loss: 0.757; train_acc: 0.685; train_f1: 0.704, val_acc: 0.567; val_f1: 0.638\n",
      "Epoch 26; train_loss: 0.589; val_loss: 0.76; train_acc: 0.693; train_f1: 0.71, val_acc: 0.617; val_f1: 0.713\n",
      "Epoch 27; train_loss: 0.578; val_loss: 0.791; train_acc: 0.701; train_f1: 0.714, val_acc: 0.608; val_f1: 0.668\n",
      "Epoch 28; train_loss: 0.583; val_loss: 0.746; train_acc: 0.693; train_f1: 0.71, val_acc: 0.592; val_f1: 0.648\n",
      "Epoch 29; train_loss: 0.571; val_loss: 0.721; train_acc: 0.699; train_f1: 0.715, val_acc: 0.65; val_f1: 0.728\n",
      "Epoch 30; train_loss: 0.57; val_loss: 0.751; train_acc: 0.706; train_f1: 0.721, val_acc: 0.592; val_f1: 0.644\n",
      "Epoch 31; train_loss: 0.558; val_loss: 0.689; train_acc: 0.72; train_f1: 0.738, val_acc: 0.658; val_f1: 0.704\n",
      "Epoch 32; train_loss: 0.564; val_loss: 0.695; train_acc: 0.721; train_f1: 0.739, val_acc: 0.65; val_f1: 0.722\n",
      "Epoch 33; train_loss: 0.557; val_loss: 0.685; train_acc: 0.725; train_f1: 0.742, val_acc: 0.658; val_f1: 0.63\n",
      "Epoch 34; train_loss: 0.551; val_loss: 0.773; train_acc: 0.729; train_f1: 0.74, val_acc: 0.625; val_f1: 0.721\n",
      "Epoch 35; train_loss: 0.55; val_loss: 0.703; train_acc: 0.717; train_f1: 0.735, val_acc: 0.617; val_f1: 0.633\n",
      "Epoch 36; train_loss: 0.55; val_loss: 0.715; train_acc: 0.738; train_f1: 0.748, val_acc: 0.642; val_f1: 0.699\n",
      "Epoch 37; train_loss: 0.546; val_loss: 0.659; train_acc: 0.727; train_f1: 0.742, val_acc: 0.633; val_f1: 0.631\n",
      "Epoch 38; train_loss: 0.536; val_loss: 0.724; train_acc: 0.741; train_f1: 0.753, val_acc: 0.617; val_f1: 0.65\n",
      "Epoch 39; train_loss: 0.517; val_loss: 0.655; train_acc: 0.755; train_f1: 0.765, val_acc: 0.658; val_f1: 0.706\n",
      "Epoch 40; train_loss: 0.54; val_loss: 0.804; train_acc: 0.732; train_f1: 0.745, val_acc: 0.625; val_f1: 0.644\n",
      "Epoch 41; train_loss: 0.533; val_loss: 0.745; train_acc: 0.738; train_f1: 0.749, val_acc: 0.633; val_f1: 0.626\n",
      "Epoch 42; train_loss: 0.522; val_loss: 0.817; train_acc: 0.749; train_f1: 0.764, val_acc: 0.575; val_f1: 0.659\n",
      "Epoch 43; train_loss: 0.521; val_loss: 0.655; train_acc: 0.742; train_f1: 0.749, val_acc: 0.7; val_f1: 0.646\n",
      "Epoch 44; train_loss: 0.523; val_loss: 0.771; train_acc: 0.753; train_f1: 0.759, val_acc: 0.575; val_f1: 0.626\n",
      "Epoch 45; train_loss: 0.518; val_loss: 0.749; train_acc: 0.753; train_f1: 0.759, val_acc: 0.617; val_f1: 0.646\n",
      "Epoch 46; train_loss: 0.506; val_loss: 0.658; train_acc: 0.772; train_f1: 0.783, val_acc: 0.642; val_f1: 0.691\n",
      "Epoch 47; train_loss: 0.503; val_loss: 0.707; train_acc: 0.771; train_f1: 0.779, val_acc: 0.667; val_f1: 0.66\n",
      "Epoch 48; train_loss: 0.509; val_loss: 0.691; train_acc: 0.765; train_f1: 0.772, val_acc: 0.658; val_f1: 0.707\n",
      "Epoch 49; train_loss: 0.49; val_loss: 0.775; train_acc: 0.773; train_f1: 0.781, val_acc: 0.592; val_f1: 0.66\n",
      "Epoch 50; train_loss: 0.497; val_loss: 0.68; train_acc: 0.781; train_f1: 0.787, val_acc: 0.65; val_f1: 0.614\n",
      "Epoch 51; train_loss: 0.502; val_loss: 0.67; train_acc: 0.769; train_f1: 0.776, val_acc: 0.683; val_f1: 0.724\n",
      "Epoch 52; train_loss: 0.49; val_loss: 0.763; train_acc: 0.774; train_f1: 0.786, val_acc: 0.642; val_f1: 0.678\n",
      "Epoch 53; train_loss: 0.491; val_loss: 0.673; train_acc: 0.767; train_f1: 0.775, val_acc: 0.667; val_f1: 0.726\n",
      "Epoch 54; train_loss: 0.489; val_loss: 0.714; train_acc: 0.784; train_f1: 0.792, val_acc: 0.65; val_f1: 0.63\n",
      "Epoch 55; train_loss: 0.487; val_loss: 0.705; train_acc: 0.772; train_f1: 0.782, val_acc: 0.65; val_f1: 0.585\n",
      "Epoch 56; train_loss: 0.488; val_loss: 0.725; train_acc: 0.775; train_f1: 0.782, val_acc: 0.642; val_f1: 0.657\n",
      "Epoch 57; train_loss: 0.486; val_loss: 0.703; train_acc: 0.79; train_f1: 0.799, val_acc: 0.642; val_f1: 0.632\n",
      "Epoch 58; train_loss: 0.48; val_loss: 0.657; train_acc: 0.782; train_f1: 0.788, val_acc: 0.692; val_f1: 0.637\n",
      "Epoch 59; train_loss: 0.49; val_loss: 0.709; train_acc: 0.785; train_f1: 0.79, val_acc: 0.667; val_f1: 0.627\n",
      "Epoch 60; train_loss: 0.468; val_loss: 0.689; train_acc: 0.784; train_f1: 0.79, val_acc: 0.683; val_f1: 0.671\n",
      "Epoch 61; train_loss: 0.478; val_loss: 0.689; train_acc: 0.788; train_f1: 0.793, val_acc: 0.667; val_f1: 0.701\n",
      "Epoch 62; train_loss: 0.477; val_loss: 0.666; train_acc: 0.776; train_f1: 0.784, val_acc: 0.683; val_f1: 0.688\n",
      "Epoch 63; train_loss: 0.477; val_loss: 0.69; train_acc: 0.789; train_f1: 0.794, val_acc: 0.7; val_f1: 0.659\n",
      "Epoch 64; train_loss: 0.469; val_loss: 0.667; train_acc: 0.787; train_f1: 0.793, val_acc: 0.7; val_f1: 0.631\n",
      "Epoch 65; train_loss: 0.469; val_loss: 0.705; train_acc: 0.786; train_f1: 0.791, val_acc: 0.675; val_f1: 0.678\n",
      "Epoch 66; train_loss: 0.479; val_loss: 0.744; train_acc: 0.782; train_f1: 0.791, val_acc: 0.6; val_f1: 0.661\n",
      "Epoch 67; train_loss: 0.476; val_loss: 0.647; train_acc: 0.78; train_f1: 0.788, val_acc: 0.692; val_f1: 0.702\n",
      "Epoch 68; train_loss: 0.468; val_loss: 0.704; train_acc: 0.795; train_f1: 0.798, val_acc: 0.7; val_f1: 0.715\n",
      "Epoch 69; train_loss: 0.464; val_loss: 0.691; train_acc: 0.792; train_f1: 0.797, val_acc: 0.692; val_f1: 0.637\n",
      "Epoch 70; train_loss: 0.448; val_loss: 0.72; train_acc: 0.802; train_f1: 0.804, val_acc: 0.692; val_f1: 0.662\n",
      "Epoch 71; train_loss: 0.458; val_loss: 0.711; train_acc: 0.789; train_f1: 0.794, val_acc: 0.675; val_f1: 0.641\n",
      "Epoch 72; train_loss: 0.456; val_loss: 0.694; train_acc: 0.796; train_f1: 0.802, val_acc: 0.683; val_f1: 0.658\n",
      "Epoch 73; train_loss: 0.451; val_loss: 0.729; train_acc: 0.8; train_f1: 0.806, val_acc: 0.625; val_f1: 0.618\n",
      "Epoch 74; train_loss: 0.465; val_loss: 0.832; train_acc: 0.784; train_f1: 0.79, val_acc: 0.633; val_f1: 0.611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75; train_loss: 0.474; val_loss: 0.735; train_acc: 0.785; train_f1: 0.786, val_acc: 0.667; val_f1: 0.674\n",
      "Epoch 76; train_loss: 0.458; val_loss: 0.753; train_acc: 0.799; train_f1: 0.801, val_acc: 0.7; val_f1: 0.669\n",
      "Epoch 77; train_loss: 0.451; val_loss: 0.71; train_acc: 0.786; train_f1: 0.791, val_acc: 0.692; val_f1: 0.679\n",
      "Epoch 78; train_loss: 0.456; val_loss: 0.826; train_acc: 0.798; train_f1: 0.801, val_acc: 0.625; val_f1: 0.637\n",
      "Epoch 79; train_loss: 0.454; val_loss: 0.735; train_acc: 0.784; train_f1: 0.79, val_acc: 0.683; val_f1: 0.642\n",
      "Epoch 80; train_loss: 0.448; val_loss: 0.728; train_acc: 0.803; train_f1: 0.806, val_acc: 0.667; val_f1: 0.696\n",
      "Epoch 81; train_loss: 0.446; val_loss: 0.717; train_acc: 0.799; train_f1: 0.809, val_acc: 0.683; val_f1: 0.689\n",
      "Epoch 82; train_loss: 0.458; val_loss: 0.777; train_acc: 0.783; train_f1: 0.786, val_acc: 0.633; val_f1: 0.643\n",
      "Epoch 83; train_loss: 0.445; val_loss: 0.689; train_acc: 0.796; train_f1: 0.801, val_acc: 0.7; val_f1: 0.684\n",
      "Epoch 84; train_loss: 0.431; val_loss: 0.722; train_acc: 0.81; train_f1: 0.814, val_acc: 0.608; val_f1: 0.566\n",
      "Epoch 85; train_loss: 0.445; val_loss: 0.835; train_acc: 0.799; train_f1: 0.801, val_acc: 0.65; val_f1: 0.648\n",
      "Epoch 86; train_loss: 0.444; val_loss: 0.802; train_acc: 0.802; train_f1: 0.806, val_acc: 0.625; val_f1: 0.666\n",
      "Epoch 87; train_loss: 0.449; val_loss: 0.747; train_acc: 0.789; train_f1: 0.799, val_acc: 0.625; val_f1: 0.616\n",
      "Epoch 88; train_loss: 0.46; val_loss: 0.774; train_acc: 0.785; train_f1: 0.787, val_acc: 0.692; val_f1: 0.719\n",
      "Epoch 89; train_loss: 0.446; val_loss: 0.765; train_acc: 0.806; train_f1: 0.808, val_acc: 0.708; val_f1: 0.72\n",
      "Epoch 90; train_loss: 0.435; val_loss: 0.749; train_acc: 0.807; train_f1: 0.812, val_acc: 0.7; val_f1: 0.655\n",
      "Epoch 91; train_loss: 0.447; val_loss: 0.795; train_acc: 0.796; train_f1: 0.8, val_acc: 0.633; val_f1: 0.656\n",
      "Epoch 92; train_loss: 0.443; val_loss: 0.742; train_acc: 0.804; train_f1: 0.806, val_acc: 0.667; val_f1: 0.596\n",
      "Epoch 93; train_loss: 0.432; val_loss: 0.934; train_acc: 0.806; train_f1: 0.812, val_acc: 0.633; val_f1: 0.675\n",
      "Epoch 94; train_loss: 0.444; val_loss: 0.809; train_acc: 0.808; train_f1: 0.81, val_acc: 0.633; val_f1: 0.651\n",
      "Epoch 95; train_loss: 0.439; val_loss: 0.743; train_acc: 0.812; train_f1: 0.814, val_acc: 0.683; val_f1: 0.689\n",
      "Epoch 96; train_loss: 0.445; val_loss: 0.816; train_acc: 0.803; train_f1: 0.802, val_acc: 0.617; val_f1: 0.657\n",
      "Epoch 97; train_loss: 0.43; val_loss: 0.82; train_acc: 0.805; train_f1: 0.807, val_acc: 0.633; val_f1: 0.624\n",
      "Epoch 98; train_loss: 0.42; val_loss: 0.774; train_acc: 0.813; train_f1: 0.819, val_acc: 0.692; val_f1: 0.619\n",
      "Epoch 99; train_loss: 0.429; val_loss: 0.958; train_acc: 0.806; train_f1: 0.81, val_acc: 0.625; val_f1: 0.643\n",
      "Epoch 100; train_loss: 0.425; val_loss: 0.689; train_acc: 0.807; train_f1: 0.812, val_acc: 0.7; val_f1: 0.636\n",
      "Epoch 101; train_loss: 0.429; val_loss: 0.834; train_acc: 0.809; train_f1: 0.813, val_acc: 0.708; val_f1: 0.672\n",
      "Epoch 102; train_loss: 0.427; val_loss: 0.717; train_acc: 0.807; train_f1: 0.808, val_acc: 0.7; val_f1: 0.715\n",
      "Epoch 103; train_loss: 0.423; val_loss: 0.76; train_acc: 0.808; train_f1: 0.811, val_acc: 0.683; val_f1: 0.696\n",
      "Epoch 104; train_loss: 0.434; val_loss: 0.73; train_acc: 0.811; train_f1: 0.817, val_acc: 0.692; val_f1: 0.709\n",
      "Epoch 105; train_loss: 0.44; val_loss: 0.703; train_acc: 0.797; train_f1: 0.802, val_acc: 0.675; val_f1: 0.626\n",
      "Epoch 106; train_loss: 0.431; val_loss: 0.755; train_acc: 0.81; train_f1: 0.812, val_acc: 0.7; val_f1: 0.733\n",
      "Epoch 107; train_loss: 0.425; val_loss: 1.128; train_acc: 0.812; train_f1: 0.817, val_acc: 0.625; val_f1: 0.641\n",
      "Epoch 108; train_loss: 0.414; val_loss: 0.767; train_acc: 0.821; train_f1: 0.827, val_acc: 0.692; val_f1: 0.583\n",
      "Epoch 109; train_loss: 0.427; val_loss: 0.792; train_acc: 0.808; train_f1: 0.803, val_acc: 0.675; val_f1: 0.669\n",
      "Epoch 110; train_loss: 0.42; val_loss: 0.752; train_acc: 0.814; train_f1: 0.82, val_acc: 0.675; val_f1: 0.691\n",
      "Epoch 111; train_loss: 0.422; val_loss: 0.746; train_acc: 0.823; train_f1: 0.824, val_acc: 0.692; val_f1: 0.691\n",
      "Epoch 112; train_loss: 0.426; val_loss: 0.82; train_acc: 0.805; train_f1: 0.809, val_acc: 0.675; val_f1: 0.615\n",
      "Epoch 113; train_loss: 0.42; val_loss: 1.03; train_acc: 0.812; train_f1: 0.818, val_acc: 0.608; val_f1: 0.652\n",
      "Epoch 114; train_loss: 0.404; val_loss: 0.922; train_acc: 0.825; train_f1: 0.828, val_acc: 0.625; val_f1: 0.633\n",
      "Epoch 115; train_loss: 0.418; val_loss: 0.79; train_acc: 0.821; train_f1: 0.822, val_acc: 0.675; val_f1: 0.638\n",
      "Epoch 116; train_loss: 0.419; val_loss: 0.845; train_acc: 0.814; train_f1: 0.814, val_acc: 0.6; val_f1: 0.556\n",
      "Epoch 117; train_loss: 0.421; val_loss: 0.952; train_acc: 0.809; train_f1: 0.813, val_acc: 0.625; val_f1: 0.631\n",
      "Epoch 118; train_loss: 0.408; val_loss: 0.789; train_acc: 0.823; train_f1: 0.824, val_acc: 0.692; val_f1: 0.633\n",
      "Epoch 119; train_loss: 0.415; val_loss: 0.807; train_acc: 0.822; train_f1: 0.828, val_acc: 0.667; val_f1: 0.601\n",
      "Epoch 120; train_loss: 0.415; val_loss: 0.788; train_acc: 0.819; train_f1: 0.818, val_acc: 0.692; val_f1: 0.704\n",
      "Epoch 121; train_loss: 0.414; val_loss: 0.812; train_acc: 0.818; train_f1: 0.817, val_acc: 0.7; val_f1: 0.71\n",
      "Epoch 122; train_loss: 0.414; val_loss: 0.767; train_acc: 0.823; train_f1: 0.826, val_acc: 0.7; val_f1: 0.71\n",
      "Epoch 123; train_loss: 0.409; val_loss: 0.879; train_acc: 0.818; train_f1: 0.82, val_acc: 0.642; val_f1: 0.668\n",
      "Epoch 124; train_loss: 0.414; val_loss: 0.853; train_acc: 0.812; train_f1: 0.819, val_acc: 0.658; val_f1: 0.714\n",
      "Epoch 125; train_loss: 0.407; val_loss: 0.828; train_acc: 0.822; train_f1: 0.824, val_acc: 0.717; val_f1: 0.667\n",
      "Epoch 126; train_loss: 0.417; val_loss: 0.782; train_acc: 0.807; train_f1: 0.811, val_acc: 0.667; val_f1: 0.658\n",
      "Epoch 127; train_loss: 0.407; val_loss: 0.834; train_acc: 0.825; train_f1: 0.826, val_acc: 0.692; val_f1: 0.693\n",
      "Epoch 128; train_loss: 0.421; val_loss: 0.828; train_acc: 0.814; train_f1: 0.818, val_acc: 0.633; val_f1: 0.64\n",
      "Epoch 129; train_loss: 0.405; val_loss: 0.76; train_acc: 0.821; train_f1: 0.825, val_acc: 0.7; val_f1: 0.691\n",
      "Epoch 130; train_loss: 0.441; val_loss: 0.815; train_acc: 0.808; train_f1: 0.81, val_acc: 0.683; val_f1: 0.712\n",
      "Epoch 131; train_loss: 0.408; val_loss: 0.767; train_acc: 0.826; train_f1: 0.825, val_acc: 0.683; val_f1: 0.612\n",
      "Epoch 132; train_loss: 0.411; val_loss: 0.809; train_acc: 0.828; train_f1: 0.83, val_acc: 0.667; val_f1: 0.587\n",
      "Epoch 133; train_loss: 0.413; val_loss: 0.892; train_acc: 0.817; train_f1: 0.816, val_acc: 0.692; val_f1: 0.64\n",
      "Epoch 134; train_loss: 0.406; val_loss: 0.728; train_acc: 0.82; train_f1: 0.823, val_acc: 0.658; val_f1: 0.598\n",
      "Epoch 135; train_loss: 0.424; val_loss: 0.76; train_acc: 0.811; train_f1: 0.813, val_acc: 0.675; val_f1: 0.683\n",
      "Epoch 136; train_loss: 0.402; val_loss: 0.78; train_acc: 0.818; train_f1: 0.823, val_acc: 0.692; val_f1: 0.661\n",
      "Epoch 137; train_loss: 0.413; val_loss: 0.819; train_acc: 0.816; train_f1: 0.817, val_acc: 0.683; val_f1: 0.722\n",
      "Epoch 138; train_loss: 0.425; val_loss: 0.826; train_acc: 0.809; train_f1: 0.809, val_acc: 0.608; val_f1: 0.594\n",
      "Epoch 139; train_loss: 0.399; val_loss: 0.774; train_acc: 0.831; train_f1: 0.829, val_acc: 0.692; val_f1: 0.686\n",
      "Epoch 140; train_loss: 0.406; val_loss: 0.861; train_acc: 0.816; train_f1: 0.82, val_acc: 0.675; val_f1: 0.657\n",
      "Epoch 141; train_loss: 0.395; val_loss: 1.025; train_acc: 0.82; train_f1: 0.823, val_acc: 0.608; val_f1: 0.662\n",
      "Epoch 142; train_loss: 0.398; val_loss: 0.784; train_acc: 0.833; train_f1: 0.834, val_acc: 0.683; val_f1: 0.682\n",
      "Epoch 143; train_loss: 0.397; val_loss: 0.802; train_acc: 0.832; train_f1: 0.837, val_acc: 0.683; val_f1: 0.664\n",
      "Epoch 144; train_loss: 0.411; val_loss: 0.82; train_acc: 0.818; train_f1: 0.817, val_acc: 0.633; val_f1: 0.681\n",
      "Epoch 145; train_loss: 0.407; val_loss: 0.905; train_acc: 0.824; train_f1: 0.824, val_acc: 0.667; val_f1: 0.709\n",
      "Epoch 146; train_loss: 0.407; val_loss: 0.847; train_acc: 0.814; train_f1: 0.814, val_acc: 0.675; val_f1: 0.723\n",
      "Epoch 147; train_loss: 0.405; val_loss: 0.84; train_acc: 0.821; train_f1: 0.816, val_acc: 0.658; val_f1: 0.631\n",
      "Epoch 148; train_loss: 0.408; val_loss: 0.795; train_acc: 0.822; train_f1: 0.827, val_acc: 0.692; val_f1: 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149; train_loss: 0.401; val_loss: 0.814; train_acc: 0.824; train_f1: 0.825, val_acc: 0.608; val_f1: 0.6\n",
      "Epoch 150; train_loss: 0.401; val_loss: 0.8; train_acc: 0.828; train_f1: 0.831, val_acc: 0.7; val_f1: 0.714\n",
      "Epoch 151; train_loss: 0.396; val_loss: 0.799; train_acc: 0.824; train_f1: 0.829, val_acc: 0.692; val_f1: 0.74\n",
      "Epoch 152; train_loss: 0.383; val_loss: 1.043; train_acc: 0.828; train_f1: 0.831, val_acc: 0.617; val_f1: 0.654\n",
      "Epoch 153; train_loss: 0.39; val_loss: 0.905; train_acc: 0.829; train_f1: 0.834, val_acc: 0.642; val_f1: 0.669\n",
      "Epoch 154; train_loss: 0.386; val_loss: 0.8; train_acc: 0.838; train_f1: 0.84, val_acc: 0.675; val_f1: 0.675\n",
      "Epoch 155; train_loss: 0.405; val_loss: 0.863; train_acc: 0.813; train_f1: 0.815, val_acc: 0.675; val_f1: 0.644\n",
      "Epoch 156; train_loss: 0.402; val_loss: 0.807; train_acc: 0.828; train_f1: 0.834, val_acc: 0.692; val_f1: 0.654\n",
      "Epoch 157; train_loss: 0.401; val_loss: 0.807; train_acc: 0.824; train_f1: 0.821, val_acc: 0.7; val_f1: 0.63\n",
      "Epoch 158; train_loss: 0.391; val_loss: 1.165; train_acc: 0.829; train_f1: 0.829, val_acc: 0.608; val_f1: 0.618\n",
      "Epoch 159; train_loss: 0.393; val_loss: 0.893; train_acc: 0.821; train_f1: 0.826, val_acc: 0.683; val_f1: 0.744\n",
      "Epoch 160; train_loss: 0.393; val_loss: 0.792; train_acc: 0.824; train_f1: 0.825, val_acc: 0.692; val_f1: 0.693\n",
      "Epoch 161; train_loss: 0.399; val_loss: 0.885; train_acc: 0.832; train_f1: 0.833, val_acc: 0.683; val_f1: 0.726\n",
      "Epoch 162; train_loss: 0.393; val_loss: 0.861; train_acc: 0.827; train_f1: 0.828, val_acc: 0.683; val_f1: 0.731\n",
      "Epoch 163; train_loss: 0.386; val_loss: 0.774; train_acc: 0.833; train_f1: 0.833, val_acc: 0.683; val_f1: 0.714\n",
      "Epoch 164; train_loss: 0.403; val_loss: 0.925; train_acc: 0.82; train_f1: 0.819, val_acc: 0.675; val_f1: 0.63\n",
      "Epoch 165; train_loss: 0.39; val_loss: 0.822; train_acc: 0.82; train_f1: 0.825, val_acc: 0.7; val_f1: 0.635\n",
      "Epoch 166; train_loss: 0.395; val_loss: 0.829; train_acc: 0.824; train_f1: 0.821, val_acc: 0.617; val_f1: 0.64\n",
      "Epoch 167; train_loss: 0.39; val_loss: 0.828; train_acc: 0.828; train_f1: 0.829, val_acc: 0.683; val_f1: 0.619\n",
      "Epoch 168; train_loss: 0.391; val_loss: 0.866; train_acc: 0.83; train_f1: 0.832, val_acc: 0.675; val_f1: 0.658\n",
      "Epoch 169; train_loss: 0.392; val_loss: 0.829; train_acc: 0.83; train_f1: 0.835, val_acc: 0.692; val_f1: 0.653\n",
      "Epoch 170; train_loss: 0.383; val_loss: 0.785; train_acc: 0.843; train_f1: 0.843, val_acc: 0.683; val_f1: 0.712\n",
      "Epoch 171; train_loss: 0.384; val_loss: 0.817; train_acc: 0.845; train_f1: 0.846, val_acc: 0.675; val_f1: 0.619\n",
      "Epoch 172; train_loss: 0.386; val_loss: 1.143; train_acc: 0.832; train_f1: 0.831, val_acc: 0.617; val_f1: 0.67\n",
      "Epoch 173; train_loss: 0.402; val_loss: 0.831; train_acc: 0.823; train_f1: 0.824, val_acc: 0.675; val_f1: 0.675\n",
      "Epoch 174; train_loss: 0.37; val_loss: 1.01; train_acc: 0.835; train_f1: 0.836, val_acc: 0.617; val_f1: 0.61\n",
      "Epoch 175; train_loss: 0.382; val_loss: 0.831; train_acc: 0.83; train_f1: 0.832, val_acc: 0.675; val_f1: 0.627\n",
      "Epoch 176; train_loss: 0.404; val_loss: 0.891; train_acc: 0.819; train_f1: 0.823, val_acc: 0.633; val_f1: 0.658\n",
      "Epoch 177; train_loss: 0.394; val_loss: 0.941; train_acc: 0.827; train_f1: 0.83, val_acc: 0.675; val_f1: 0.651\n",
      "Epoch 178; train_loss: 0.391; val_loss: 0.846; train_acc: 0.828; train_f1: 0.829, val_acc: 0.683; val_f1: 0.623\n",
      "Epoch 179; train_loss: 0.386; val_loss: 0.761; train_acc: 0.838; train_f1: 0.843, val_acc: 0.692; val_f1: 0.715\n",
      "Epoch 180; train_loss: 0.39; val_loss: 0.946; train_acc: 0.836; train_f1: 0.834, val_acc: 0.658; val_f1: 0.651\n",
      "Epoch 181; train_loss: 0.38; val_loss: 0.928; train_acc: 0.838; train_f1: 0.839, val_acc: 0.642; val_f1: 0.677\n",
      "Epoch 182; train_loss: 0.374; val_loss: 0.841; train_acc: 0.843; train_f1: 0.847, val_acc: 0.642; val_f1: 0.646\n",
      "Epoch 183; train_loss: 0.394; val_loss: 0.834; train_acc: 0.826; train_f1: 0.831, val_acc: 0.683; val_f1: 0.677\n",
      "Epoch 184; train_loss: 0.392; val_loss: 0.799; train_acc: 0.824; train_f1: 0.825, val_acc: 0.692; val_f1: 0.612\n",
      "Epoch 185; train_loss: 0.387; val_loss: 0.85; train_acc: 0.833; train_f1: 0.834, val_acc: 0.683; val_f1: 0.662\n",
      "Epoch 186; train_loss: 0.39; val_loss: 0.848; train_acc: 0.83; train_f1: 0.83, val_acc: 0.683; val_f1: 0.685\n",
      "Epoch 187; train_loss: 0.391; val_loss: 0.837; train_acc: 0.832; train_f1: 0.831, val_acc: 0.692; val_f1: 0.637\n",
      "Epoch 188; train_loss: 0.395; val_loss: 0.792; train_acc: 0.819; train_f1: 0.818, val_acc: 0.683; val_f1: 0.656\n",
      "Epoch 189; train_loss: 0.369; val_loss: 1.067; train_acc: 0.837; train_f1: 0.835, val_acc: 0.642; val_f1: 0.671\n",
      "Epoch 190; train_loss: 0.373; val_loss: 0.827; train_acc: 0.833; train_f1: 0.834, val_acc: 0.667; val_f1: 0.623\n",
      "Epoch 191; train_loss: 0.39; val_loss: 0.818; train_acc: 0.832; train_f1: 0.83, val_acc: 0.625; val_f1: 0.64\n",
      "Epoch 192; train_loss: 0.39; val_loss: 0.952; train_acc: 0.835; train_f1: 0.833, val_acc: 0.633; val_f1: 0.652\n",
      "Epoch 193; train_loss: 0.389; val_loss: 0.89; train_acc: 0.84; train_f1: 0.834, val_acc: 0.675; val_f1: 0.671\n",
      "Epoch 194; train_loss: 0.378; val_loss: 0.816; train_acc: 0.844; train_f1: 0.848, val_acc: 0.683; val_f1: 0.702\n",
      "Epoch 195; train_loss: 0.387; val_loss: 0.81; train_acc: 0.83; train_f1: 0.834, val_acc: 0.683; val_f1: 0.72\n",
      "Epoch 196; train_loss: 0.396; val_loss: 0.863; train_acc: 0.835; train_f1: 0.832, val_acc: 0.683; val_f1: 0.658\n",
      "Epoch 197; train_loss: 0.392; val_loss: 0.826; train_acc: 0.831; train_f1: 0.834, val_acc: 0.675; val_f1: 0.667\n",
      "Epoch 198; train_loss: 0.369; val_loss: 0.795; train_acc: 0.846; train_f1: 0.848, val_acc: 0.7; val_f1: 0.653\n",
      "Epoch 199; train_loss: 0.372; val_loss: 1.041; train_acc: 0.837; train_f1: 0.836, val_acc: 0.633; val_f1: 0.638\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr= 0.00001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    count = 0\n",
    "    train_acc = 0\n",
    "    train_f1 = 0\n",
    "    for item in train_dataloader:\n",
    "        x, y = item[:, :-1], item[:, -1].reshape(-1, 1)\n",
    "        y = y.double().to(device)\n",
    "        x = x.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_tilde = model(x)\n",
    "        y_tilde = y_tilde.double().to(device)\n",
    "        loss  = criterion(y_tilde, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc, prec, recall, f1 = compute_measures(y_tilde, y)\n",
    "        train_acc += acc\n",
    "        train_f1+=f1\n",
    "        \n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    count = 0\n",
    "    valid_acc = 0\n",
    "    valid_f1 = 0\n",
    "    for item in valid_dataloader:\n",
    "        with torch.no_grad():\n",
    "            x, y = item[:, :-1], item[:, -1].reshape(-1, 1)\n",
    "            y = y.double().to(device)\n",
    "            x = x.float().to(device)\n",
    "            y_tilde = model(x)\n",
    "            y_tilde = y_tilde.double().to(device)\n",
    "            loss  = criterion(y_tilde, y)\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            acc, prec, recall, f1 = compute_measures(y_tilde, y)\n",
    "            valid_acc += acc\n",
    "            valid_f1+=f1\n",
    "        \n",
    "    print(\"Epoch {}; train_loss: {}; val_loss: {}; train_acc: {}; train_f1: {}, val_acc: {}; val_f1: {}\".format(epoch, \n",
    "                                                                                  np.round(train_loss/len(train_dataloader), decimals=3), \n",
    "                                                                                  np.round(valid_loss/len(valid_dataloader), decimals=3), \n",
    "                                                                                  np.round(train_acc/len(train_dataloader), decimals=3), \n",
    "                                                                                  np.round(train_f1/len(train_dataloader), decimals=3),\n",
    "                                                                                  np.round(valid_acc/len(valid_dataloader), decimals=3),\n",
    "                                                                                  np.round(valid_f1/len(valid_dataloader), decimals=3)\n",
    "                                                                                  ))\n",
    "                                        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584d2dc",
   "metadata": {},
   "source": [
    "## Running on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57f387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "gt = []\n",
    "\n",
    "for item in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        x, y = item[:, :-1], item[:, -1].reshape(-1, 1)\n",
    "        y = y.double().to(device)\n",
    "        x = x.float().to(device)\n",
    "        y_tilde = model(x)\n",
    "        y_tilde = y_tilde.double().to(device)\n",
    "        y_pred = np.round(y_tilde.to('cpu').detach().numpy(), decimals = 0)\n",
    "        y = np.round(y.to('cpu').detach().numpy(), decimals = 0)\n",
    "        y = list(y)\n",
    "        y_pred = list(y_pred)\n",
    "        gt.extend(y)\n",
    "        preds.extend(y_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bae189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(gt, preds)\n",
    "prec = precision_score(gt, preds)\n",
    "recall = recall_score(gt, preds)\n",
    "f1 = f1_score(gt, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730e5e0",
   "metadata": {},
   "source": [
    "##### ACC - 67 gan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18da9a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.703125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca300731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86b6cef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
